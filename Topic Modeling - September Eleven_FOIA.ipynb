{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the modules and packages\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Acessing and reading the files\n",
    "file1 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC1229684.doc\")\n",
    "file2 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC1525482.doc\")\n",
    "file3 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC6184107.doc\")\n",
    "file4 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC6220800.doc\")\n",
    "file5 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC6297294.doc\")\n",
    "file6 = os.path.join(\"C:/Users\\\\GU\\\\MasterDegree\\\\mmd\\\\mmddocs\", \"DOC6364591.doc\")\n",
    "\n",
    "f_object1 = open(file1, \"r\")\n",
    "doc1 = f_object1.read()\n",
    "f_object1.close()\n",
    "\n",
    "f_object2 = open(file2, \"r\")\n",
    "doc2 = f_object2.read()\n",
    "f_object2.close()\n",
    "\n",
    "f_object3 = open(file3, \"r\")\n",
    "doc3 = f_object3.read()\n",
    "f_object3.close()\n",
    "\n",
    "f_object4 = open(file4, \"r\")\n",
    "doc4 = f_object4.read()\n",
    "f_object4.close()\n",
    "\n",
    "f_object5 = open(file5, \"r\")\n",
    "doc5 = f_object5.read()\n",
    "f_object5.close()\n",
    "\n",
    "f_object6 = open(file6, \"r\")\n",
    "doc6 = f_object6.read()\n",
    "f_object6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gathering the docs\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5, doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the docs\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.061*\"spsn\" + 0.029*\"par\" + 0.026*\"0\"'), (1, '0.012*\"cs27lang1033langfe1033b0i0ul0strike0scaps0fs22afs22charscalex100expndtw0cf1dn0\" + 0.006*\"community\" + 0.006*\"intelligence\"'), (2, '0.000*\"spsn\" + 0.000*\"0\" + 0.000*\"par\"'), (3, '0.011*\"cs23lang1033langfe1033b0i0ul0strike0scaps0fs24afs24charscalex100expndtw10cf1dn0\" + 0.011*\"par\" + 0.008*\"report\"'), (4, '0.177*\"spsn\" + 0.088*\"0\" + 0.047*\"par\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
